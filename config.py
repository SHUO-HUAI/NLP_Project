stories_path = './data'
tokenized_path = './data_token'

all_tokenized_path = './data_all_token/'
split_file = {'test': './test_all.txt', 'train': './train_all.txt', 'val': './val_all.txt'}
load_data = './data_finished_verytiny'
resume = None
epoch = 100
save = './logs-checkpoint-save'
batch_size = 64