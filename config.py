stories_path = './data'
tokenized_path = './data_token'

all_tokenized_path = './data_all_token/'
split_file = {'test': './test_all.txt', 'train': './train_all.txt', 'val': './val_all.txt'}
load_data = './data_finished_30k_tiny_x2'
resume = None
epoch = 25
save = './logs-checkpoint-save'
batch_size = 32
lr = 0.01